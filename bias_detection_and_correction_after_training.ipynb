{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "# About this Notebook\n",
    "\n",
    "This notebook demonstrates how to use fairlearn and AIF360 after training. Note these are general examples and not specifically for an LLM or any network that is. The used networks are just used as a very simple examples without much overhead to explain the underlying mechanisms. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "__Equalized Odds Post-processing (Fairlearn)__: This is a fairness-enhancing post-processing technique that solves a linear program to find probabilities with which to change predicted labels to optimize Equalized Odds (a fairness metric). It ensures that, for every group identified by the sensitive feature(s), the error rates for predicting the positive outcome are approximately equal, and similarly for predicting the negative outcome. [Link for more info](https://fairlearn.org/)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "__CalibratedEqOddsPostprocessing (Fairlearn)__: This is another post-processing technique that tries to ensure fairness across different groups as defined by a sensitive feature. It uses a test set to derive a transformation that adjusts the output of a binary classifier to satisfy the 'equalized odds' criterion, i.e., it aims to have equal false positive and false negative rates for different groups. This adjustment is based on the classifier's scores on the test set, not the true labels.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "__Reject Option Classification (aif360)__: Reject Option Classification is a post-processing technique for machine learning models that introduces a 'reject option'. If a prediction falls within a certain region where the classifier is less certain, it chooses not to make a decision, thereby potentially improving fairness by avoiding uncertain predictions that may be biased. [Link to library documentation](https://aif360.readthedocs.io/en/latest/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "__Statistical Parity Difference (aif360__): This is a measure of fairness used in fair machine learning. Statistical parity difference is the difference in the probability of positive outcomes for the unprivileged and privileged groups. If the statistical parity difference is zero, it implies that both groups have the same probability of getting positive outcomes (which is often interpreted as the decision process being fair with respect to the groups). However, it's worth noting that this interpretation assumes that the two groups are otherwise identical, which might not be the case in complex real-world scenarios. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "from fairlearn.reductions import ExponentiatedGradient, DemographicParity, EqualizedOdds\n",
    "from fairlearn.postprocessing import ThresholdOptimizer\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from aif360.datasets import BinaryLabelDataset\n",
    "from aif360.algorithms.postprocessing import RejectOptionClassification\n",
    "from aif360.metrics import BinaryLabelDatasetMetric\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Equalized Odds Post-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Data Generation\n",
    "\n",
    "This section generates a synthetic dataset of 200 instances with 5 features each, out of which 3 are informative. In this context, we are assuming that the first feature is the sensitive attribute (e.g., gender), which we are trying to protect against unfair decisions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Generate a sample dataset of 200 people applying for credit\n",
    "np.random.seed(42)\n",
    "X, y = make_classification(n_samples=200, n_features=5, n_informative=3, n_classes=2, random_state=42)\n",
    "\n",
    "# Let's say the first feature is gender, which is a sensitive feature in our scenario\n",
    "sensitive_features = X[:, 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Data Splitting\n",
    "\n",
    "The dataset is split into training and test sets. The train_test_split function is used to randomly split the data, with 80% of the instances used for training the model, and the remaining 20% for testing the model's performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Split the dataset and the sensitive features into train and test sets\n",
    "X_train, X_test, y_train, y_test, sensitive_features_train, sensitive_features_test = train_test_split(X, y, sensitive_features, test_size=0.2, random_state=42)\n",
    "\n",
    "test = train_test_split(X, y, sensitive_features, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Model Definition and Training\n",
    "\n",
    "\n",
    "Here, a logistic regression model is defined and then trained using the training data. We use the ExponentiatedGradient method with a demographic parity constraint, aiming for similar selection rates across different groups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Define a logistic regression estimator\n",
    "estimator = LogisticRegression()\n",
    "\n",
    "# Define the ExponentiatedGradient instance with a demographic parity constraint\n",
    "unmitigated_predictor = ExponentiatedGradient(estimator=estimator, constraints=DemographicParity())\n",
    "unmitigated_predictor.fit(X_train, y_train, sensitive_features=sensitive_features_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Model Prediction and Evaluation (Before Post-processing)\n",
    "\n",
    "\n",
    "The trained model is then used to predict the outcomes for the test data. The prediction performance is evaluated by computing the confusion matrix, which shows the number of true positives, true negatives, false positives, and false negatives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Predict on the test set\n",
    "y_pred = unmitigated_predictor.predict(X_test)\n",
    "\n",
    "# Compute the confusion matrix before Equalized Odds post-processing\n",
    "conf_matrix_before = confusion_matrix(y_test, y_pred)\n",
    "print(\"Confusion matrix before Equalized Odds post-processing:\\n\", conf_matrix_before)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Model Training with Equalized Odds Post-processing\n",
    "\n",
    "This section applies the ExponentiatedGradient algorithm again but this time with an \"Equalized Odds\" constraint, which ensures equal false positive and true positive rates across different groups. This algorithm is trained on the training set with an L2-constraint on group size difference (`eps`) and a maximum number of iterations (`max_iter`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Apply Equalized Odds post-processing using the ExponentiatedGradient algorithm with an L2-constraint on group size difference\n",
    "constraint = EqualizedOdds()\n",
    "eps = 0.1\n",
    "max_iter = 1000\n",
    "eopp = ExponentiatedGradient(estimator=estimator, constraints=constraint, eps=eps, max_iter=max_iter)\n",
    "eopp.fit(X_train, y_train, sensitive_features=sensitive_features_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Model Prediction and Evaluation (After Post-processing)\n",
    "\n",
    "The model trained with Equalized Odds post-processing is then used to predict the outcomes for the test data. The prediction performance is again evaluated by computing the confusion matrix, showing how the model's predictions have changed after applying the fairness enhancement method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "id": "iEMv8Blc6LfU",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix before Equalized Odds post-processing:\n",
      " [[10 12]\n",
      " [ 8 10]]\n",
      "Confusion matrix after Equalized Odds post-processing:\n",
      " [[14  8]\n",
      " [ 5 13]]\n"
     ]
    }
   ],
   "source": [
    "# Predict on the test set after post-processing\n",
    "y_pred_post = eopp.predict(X_test)\n",
    "\n",
    "# Compute the confusion matrix after post-processing\n",
    "conf_matrix_after = confusion_matrix(y_test, y_pred_post)\n",
    "print(\"Confusion matrix after Equalized Odds post-processing:\\n\", conf_matrix_after)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## CalibratedEqOddsPostprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Applying Trained Logistic Regression Model on Training Data\n",
    "\n",
    "This line applies the previously trained logistic regression model to the training data to generate predicted scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Apply the logistic regression model to the training data\n",
    "dataset_train_copy.scores = unmitigated_predictor.predict(dataset_train_copy.features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Creating the Predicted Training Dataset\n",
    "\n",
    "This code create a copy of the original training dataset and replace its labels with the predicted scores to form a predicted dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Create the predicted dataset\n",
    "dataset_train_pred = dataset_train_copy.copy()\n",
    "dataset_train_pred.labels = dataset_train_copy.scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Defining Calibrated Equal Odds Post-processing Instance\n",
    "\n",
    "Here, we define an instance of the Calibrated Equal Odds Post-processing (CEOP) class, specifying unprivileged and privileged groups, and setting the cost constraint to `false positive rate (fpr)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Define the CalibratedEqOddsPostprocessing instance\n",
    "cpp = CalibratedEqOddsPostprocessing(unprivileged_groups=unprivileged_groups, \n",
    "                                     privileged_groups=privileged_groups, \n",
    "                                     cost_constraint='fpr')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Fitting Calibrated Equal Odds Post-processing Instance\n",
    "\n",
    "This following line fits the CEOP instance to the original and the predicted training datasets, calibrating the post-processing steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Fit the CEOP instance using the train dataset and the scores predicted by the unmitigated predictor\n",
    "cpp = cpp.fit(dataset_train_copy, dataset_train_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Applying Trained Logistic Regression Model on Test Data\n",
    "\n",
    "Similarly to step 1, we use the logistic regression model to generate predicted scores for the test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Apply the logistic regression model to the test data\n",
    "dataset_test_copy.scores = unmitigated_predictor.predict(dataset_test_copy.features)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Creating the Predicted Test Dataset\n",
    "\n",
    "We create a predicted test dataset by replacing the labels in a copy of the original test dataset with the predicted scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Create the predicted dataset for test data\n",
    "dataset_test_pred = dataset_test_copy.copy()\n",
    "dataset_test_pred.labels = dataset_test_copy.scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Bias Mitigation using Calibrated Equal Odds Post-processing\n",
    "\n",
    "We use the fitted CEOP instance to generate predictions on the test dataset, mitigating bias as per the calibrated post-processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Predict using the fitted CEOP instance to mitigate bias\n",
    "dataset_transf_test = cpp.predict(dataset_test_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Comparison of Model Performance before and after Post-processing\n",
    "\n",
    "Lastly, we compare the fairness metrics, particularly accuracy, before and after the post-processing step. This lets us evaluate the impact of our bias mitigation process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score before post-processing:  1.0\n",
      "Accuracy score after post-processing:  0.45\n"
     ]
    }
   ],
   "source": [
    "# Compare the fairness metrics before and after the post-processing\n",
    "y_pred_before = dataset_test_copy.labels\n",
    "y_pred_after = dataset_transf_test.labels\n",
    "\n",
    "print(\"Accuracy score before post-processing: \", accuracy_score(dataset_test_copy.labels, y_pred_before))\n",
    "print(\"Accuracy score after post-processing: \", accuracy_score(dataset_test_copy.labels, y_pred_after))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Statistical Parity Difference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Synthetic Dataset Creation\n",
    "\n",
    "This code generates a synthetic dataset with 500 samples and 2 informative features. The weights parameter ensures a class distribution of 60%-40%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Create a synthetic dataset\n",
    "np.random.seed(0)\n",
    "X, y = make_classification(n_samples=500, n_features=2, n_informative=2, n_redundant=0, weights=[0.6, 0.4], random_state=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Sensitive Attribute Definition and Dataset Split\n",
    "\n",
    "This section of the code defines the sensitive attribute as whether 'Income' is above or below its median value. It also splits the dataset into training and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Define the sensitive feature as being whether the 'Income' is above or below its median\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "sensitive_attr_train = (X_train[:, 0] > np.median(X[:, 0])).astype(int)\n",
    "sensitive_attr_test = (X_test[:, 0] > np.median(X[:, 0])).astype(int)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### DataFrame Creation\n",
    "\n",
    "This code converts the training and test data into pandas DataFrames for easier data manipulation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Create dataframes\n",
    "df_train = pd.DataFrame(X_train, columns=['Income', 'Employment Years'])\n",
    "df_train['Credit Approval'] = y_train\n",
    "df_train['Income'] = sensitive_attr_train\n",
    "\n",
    "df_test = pd.DataFrame(X_test, columns=['Income', 'Employment Years'])\n",
    "df_test['Credit Approval'] = y_test\n",
    "df_test['Income'] = sensitive_attr_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### BinaryLabelDataset Creation\n",
    "\n",
    "In the following, the dataframes are converted into BinaryLabelDataset instances which is the required format for Fairlearn's bias mitigation tools."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Create BinaryLabelDatasets\n",
    "dataset_train = BinaryLabelDataset(df=df_train, label_names=['Credit Approval'], protected_attribute_names=['Income'], favorable_label=1, unfavorable_label=0)\n",
    "dataset_test = BinaryLabelDataset(df=df_test, label_names=['Credit Approval'], protected_attribute_names=['Income'], favorable_label=1, unfavorable_label=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Classifier Training\n",
    "\n",
    "A logistic regression classifier is trained on the training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Train a classifier\n",
    "classifier = LogisticRegression(solver='liblinear')\n",
    "classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "###  Prediction Probability Calculation\n",
    "\n",
    "The trained classifier is applied to the test set to generate predicted probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Apply the classifier to the test set to get predicted probabilities\n",
    "y_pred_prob = classifier.predict_proba(X_test)[:,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Creation of Dataset with Predicted Probabilities\n",
    "\n",
    "The predicted probabilities are added to the test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Create dataset with predicted probabilities\n",
    "dataset_test_pred_prob = dataset_test.copy(deepcopy=True)\n",
    "dataset_test_pred_prob.scores = y_pred_prob.reshape(-1,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### ROC Instance Definition\n",
    "\n",
    "A Reject Option Classification (ROC) instance is defined with the specified parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Define the ROC instance\n",
    "roc = RejectOptionClassification(unprivileged_groups=[{'Income': 0}], privileged_groups=[{'Income': 1}], \n",
    "                                 low_class_thresh=0.01, high_class_thresh=0.99, num_class_thresh=100, num_ROC_margin=50,\n",
    "                                 metric_name=\"Statistical parity difference\",\n",
    "                                 metric_ub=0.05, metric_lb=-0.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### ROC Instance Fitting and Application\n",
    "\n",
    "The ROC instance is then fit to the test dataset and the dataset with predicted probabilities. The fitted instance is then used to predict the test set labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Fit the ROC instance and apply to the predicted labels\n",
    "roc = roc.fit(dataset_test, dataset_test_pred_prob)\n",
    "dataset_test_pred_transf = roc.predict(dataset_test_pred_prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Fairness Metrics Calculation\n",
    "\n",
    "This final section compares the statistical parity difference before and after the ROC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Statistical parity difference before ROC: 0.040000000000000036\n",
      "Statistical parity difference after ROC: 0.0\n"
     ]
    }
   ],
   "source": [
    "# Compare the fairness metrics before and after transformation\n",
    "metric_test_bef = BinaryLabelDatasetMetric(dataset_test_pred_prob,\n",
    "                                           unprivileged_groups=[{'Income': 0}],\n",
    "                                           privileged_groups=[{'Income': 1}])\n",
    "\n",
    "metric_test_aft = BinaryLabelDatasetMetric(dataset_test_pred_transf,\n",
    "                                           unprivileged_groups=[{'Income': 0}],\n",
    "                                           privileged_groups=[{'Income': 1}])\n",
    "\n",
    "print(\"Statistical parity difference before ROC:\", metric_test_bef.statistical_parity_difference())\n",
    "print(\"Statistical parity difference after ROC:\", metric_test_aft.statistical_parity_difference())\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
